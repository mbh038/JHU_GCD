sapply(flags,class)
vaply(falgs,class,character(1))
vapply(falgs,class,character(1))
vapply(flags,class,character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(falgs$animate,flags$landmass,mean)
tapply(flags$animate,flags$landmass,mean)
tapply(flags$population,flags$red,summary)
tapply(flags$population,flags$landmass,summary)
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants,10)
tail(plants,15
)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
?sample()
?sample
sample(1:6,4,replace=TRUE)
play()
sample(1:6,4,replace=FALSE)
sample(1:6,6,replace=FALSE)
sample(1:6,7,replace=FALSE)
sample(1:6,7,replace=TRUE)
nxt()
sample(1:6,4,replace=TRUE)
sample(1:20,10)
LETTERS
letters
sample(LETTERS)
sample(c(0,1),100,prob=c(0.3,0.7))
flips<-sample(c(0,1),100,prob=c(0.3,0.7),replace=TRUE)
flips
sum(flips)
?rbinom
rbinom(1,size=100,prob=0.7)
rbinom(100,size=1,prob=0.7)
flip2<-rbinom(100,size=1,prob=0.7)
flips2<-rbinom(100,size=1,prob=0.7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(100,25)
rnorm(10,100,25)
?rpois
rpois(5,10)
my_pois<-replicate(100,rpois(5,10))
my_pois
cm<-colmeans(my_pois)
cm<-colMeans(my_pois)
hist(cm)
dl<-Sys.Date()
d1<-Sys.Date()
class(d1)
unclass(d1)
d1
d2<-as.Date("1969-01-01")
unclass(d2)
t1<-Sys.time()
t1
class(t1)
unclass(t1)
as.POSIXlt(Sys.time())
t2<-as.POSIXlt(Sys.time())
class(t2)
unclass(t2)
t2
unclass(t2)
str(unclass(t2)
)
t2$min
weekdays(t1)
weekdays(d1)
months(t1)
quarters(t2)
t3<-"October 17,1986 08:24"
t3<-"October 17,1986, 08:24"
t3<-"October 17, 1986 08:24"
t4<-strptime(t3,"%B,%d,%Y %H;%M")
t4<-strptime(t3,"%B %d,%Y %H;%M")
t4<-strptime(t3,"%B %d,%Y %H:%M")
t4<-strptime(t3, "%B %d, %Y %H:%M")
print(t4)
t4
class(t4)
Sys.time()>t1
Sys.time(0-t1)
Sys.time()-t1
difftime(Sys.time(),t1,units='days')
data(cars)
help(cars)
head(cars)
plot(cars)
?plot
plot(x=cars$speed,y=cars$dist)
plot(x=cars$dist,y=cars$speed)
plot(x=cars$speed,y=cars$dist)
plot(x=cars$speed,y=cars$dist,Xlab="Speed")
plot(x=cars$speed,y=cars$dist,xlab="Speed")
plot(x=cars$speed,y=cars$dist,xlab="Speed",ylab="Stopping Distance")
plot(x=cars$speed,y=cars$dist,ylab="Stopping Distance")
plot(x=cars$speed,y=cars$dist,xlab="Speed",ylab="Stopping Distance")
plot(cars,Title="My Plot")
plot(cars,main="My Plot")
play()
?plot
nxt()
plot(cars,sub="My Plot Subtitle")
asa
plot(cars)
plot(cars,col=2)
plot(cars,xlim=c(10,15))
plot(cars,pch=2)
data(mtcars)
play()
dim(mtcars)
str(mtcars)
summary(mtcars)
names(mtcars)
class(mtcars)
nxt()
?boxplot
boxplot(mpg~cyl,data=mtcars)
hist(mtcars$mpg)
f<-function(x){}
my.vec<-c(NA,0,2,3,4,NA)
min(my.vec)
install.packages(lubridate)
install.packages("lubridate")
library(lubridate)
ymd("20140108")
ymd("20140108")
mdy("05072015")
install.packages("httr")
install.packages("httpuv")
install.packages("dplyr")
library(httr)
library(httpuv)
library(dplyr)
# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")
# 2. To make your own application, register at at
#    https://github.com/settings/applications. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#
#    Replace your key and secret below.
myapp <- oauth_app("github",
key = "5d271b52351635520bd9",
secret = "a149c55ab80572469ebe313609845ddf66b62325")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp,cache="FALSE")
# 4. Use API
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
library(jsonlite)
json1 = content(req)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
str(json2)
names(json2)
library(jsonlite)
json1 = content(req)
json2 = jsonlite::fromJSON(toJSON(json1))
names(json2)
select(json2,name,created_at)
library(jsonlite)
json1 = content(req)
json2 = jsonlite::fromJSON(toJSON(json1))
names(json2)
df<-select(json2,name,created_at)
df
match("datasharing",df$name)
library(jsonlite)
json1 = content(req)
json2 = jsonlite::fromJSON(toJSON(json1))
names(json2)
df<-select(json2,name,created_at)
df
pos<-match("datasharing",df$name)
time_created<-df$created_at[pos]
print(paste("The datasharing repo was created at",time_created))
?content
con=url(http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en)
htmlCode=readlines(con)
close(con)
htmlCode
con=url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode=readlines(con)
close(con)
htmlCode
?readlines
??readlines
con=url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode=readLines(con)
close(con)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=T)
xpathSApply(html, "//title", xmlValue)
install.packages("XML")
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=T)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
library(httr); html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
xpathSApply(parsedHtml , "//td[@id='col-citedby']", xmlValue)
pg1 = GET("http://httpbin.org/basic-auth/user/passwd")
pg1
pg2 = GET("http://httpbin.org/basic-auth/user/passwd",
authenticate("user","passwd"))
pg2
names(pg2)
str(pg2)
str(pg2[1])
str(pg2[2])
str(pg2[3])
str(pg2[3])
content3=content(pg2)
content3
content3=content(pg2,as="text")
content3
?content
content(pg2)
content(pg2$cookies)
content(pg2[5])
content3=content(pg2[5])
content3=content(pg2[5])
content2 = content(pg2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
google = handle("http://google.com")
pg1 = GET(handle=google,path="/")
pg2 = GET(handle=google,path="search")
pg2 = GET("http://httpbin.org/basic-auth/user/passwd",
authenticate("user","passwd"))
pg2
names (pg2)
url<-http://biostat.jhsph.edu/~jleek/contact.html
library(httr); html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
url<-"http://biostat.jhsph.edu/~jleek/contact.html"
library(httr); html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
str(parsedHTML)
str(parsedHtml)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
library(httr)
url<-"http://biostat.jhsph.edu/~jleek/contact.html"
html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
#xpathSApply(parsedHtml , "//td[@id='col-citedby']", xmlValue)
library(httr)
url<-"http://biostat.jhsph.edu/~jleek/contact.html"
html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
xpathSApply(parsedHtml , "//td[@id='col-citedby']", xmlValue)
parsedHtml
library(XML)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlParse(fileURL)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlParse(fileUrl)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
class(doc)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
class(d0c)
class(doc)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
xmlChildren(rootNode)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl)
rootNode <- xmlRoot(doc)
rootname<-xmlName(rootNode)
sapply(rootname,names)
class(rootname)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl)
rootNode <- xmlRoot(doc)
rootname<-xmlName(rootNode)
sapply(rootNode,names)
class(d0c)
class(doc)
class(rootNode)
rootchildren<-xmlChildren(rootNode)
class(rootChildren)
rootChildren<-xmlChildren(rootNode)
rm(rootchildren)
class(rootChildren)
rootChildren
sapply(rootChildren,names)
xmlSApply(rootNode,names)
class(doc)
getNodeSet(doc, "//")
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl)
class(doc)
rootNode <- xmlRoot(doc)
rootname<-xmlName(rootNode)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
class(doc)
dat<-library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
class(doc)
library(XML)
fileUrl <- "http://biostat.jhsph.edu/~jleek/contact.html"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
class(doc)
dat<- xpathSApply(doc,"//",xmlValue)
dat
dT
DAT
A
dat
str(dat)
doc
str(dat)
head(dat)
tail(dat)
head
str(doc)
head(doc)
doc[15]
rm(list=ls())
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlcode = readLines(con)
close(con)
htmlcode[15]
htmlcode[10]
nachar(htmlcode[10])
nchar(htmlcode[10])
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlcode = readLines(con)
close(con)
nchar(htmlcode[10])
nchar(htmlcode[20])
nchar(htmlcode[30])
nchar(htmlcode[100])
#download data if not yet already done so
if(!file.exists("./getdata-wksst8110.for")){
#download a data file as csv in the "data" dir
#on Mac, need method="curl" if url starts with https:
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileURL,destfile="./data/getdata-wksst8110.for")
#include date of download
datedownloaded<-date()
datedownloaded
}
#download data if not yet already done so
if(!file.exists("./data/getdata-wksst8110.for")){
#download a data file as csv in the "data" dir
#on Mac, need method="curl" if url starts with https:
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileURL,destfile="./data/getdata-wksst8110.for")
#include date of download
datedownloaded<-date()
datedownloaded
}
getwd()
setwd("H:/Rspace/JHU_Data_Science/JHU_GCD/Week2")
#download data if not yet already done so
if(!file.exists("./data/getdata-wksst8110.for")){
#download a data file as csv in the "data" dir
#on Mac, need method="curl" if url starts with https:
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileURL,destfile="./data/getdata-wksst8110.for")
#include date of download
datedownloaded<-date()
datedownloaded
}
#download data if not yet already done so
if(!file.exists("./data/getdata-wksst8110.for")){
#download a data file as csv in the "data" dir
#on Mac, need method="curl" if url starts with https:
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileURL,destfile="./data/getdata-wksst8110.for")
#include date of download
datedownloaded<-date()
datedownloaded
}
?read.fwf
df<-read.fwf(destfile,widths=13,header=TRUE)
df<-read.fwf("./data/getdata-wksst8110.for",widths=13,header=TRUE)
str(df)
head(da)
head(df)
df<-read.fwf("./data/getdata-wksst8110.for",widths=c(14,rep(c(4,9),4)),header=TRUE)
df<-read.fwf("./data/getdata-wksst8110.for",widths=c(14,4,9,4,9,4,9,4,9),header=TRUE)
df<-read.fwf("./data/getdata-wksst8110.for",
widths=c(14,4,9,4,9,4,9,4,9),
header=TRUE,
skip=3)
df<-read.fwf("./data/getdata-wksst8110.for",
widths=c(14,4,9,4,9,4,9,4,9),
header=TRUE,
skip=3,
sep="")
?read.table
df<-read.fwf("./data/getdata-wksst8110.for",
widths=c(15,4,9,4,9,4,9,4,9),
header=TRUE,
skip=3,
sep="")
df<-read.fwf("./data/nohead.for",
widths=c(15,4,9,4,9,4,9,4,9),
header=TRUE,
skip=0,
sep="")
df<-read.fwf("./data/nohead.for",
widths=c(15,4,9,4,9,4,9,4,4),
header=TRUE,
skip=0,
sep="")
?read.fwf
?read.fwf
df<-read.fwf("./data/nohead.for",
widths=c(12,4,9,4,9,4,9,4,4),
header=FALSE,
skip=0,
sep="")
widths=c(12, 7,4, 9,4, 9,4, 9,4)
df<-read.fwf("./data/nohead.for",
widths=c(12, 7,4, 9,4, 9,4, 9,4),
header=FALSE,
skip=0,
sep="")
df<-read.fwf(file="https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for",
widths=c(12, 7,4, 9,4, 9,4, 9,4),
header=FALSE,
skip=4,
)
str(df)
sum(V4)
sum(df$V4)
sum(df[,4])
df<-read.fwf(file="https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for",
widths=c(12, 7,4, 9,4, 9,4, 9,4),
header=FALSE,
skip=4,
)
sum(df[,4])
df<-read.fwf(file="https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for",
widths=c(15, 4,4, 9,4, 9,4, 9,4),
header=FALSE,
skip=4,
)
sum(df[,4])
#download data if not yet already done so and have a look at it in NotePad
if(!file.exists("./data/qf.for")){
#download a data file as csv in the "data" dir
#on Mac, need method="curl" if url starts with https:
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(fileURL,destfile="./data/q5.for")
#include date of download
datedownloaded<-date()
datedownloaded
}
# column widths are c(15, 4,4, 9,4, 9,4, 9,4)
df<-read.fwf(file="https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for",
widths=c(15, 4,4, 9,4, 9,4, 9,4),
header=FALSE,
skip=4,
)
sum(df[,4])
